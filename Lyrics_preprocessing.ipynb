{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyrics preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKsHvtZvypylPUP6T3eLPv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlgaSeleznova/ML_toolbox/blob/main/Lyrics_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl8fwi5nAp1z"
      },
      "source": [
        "# Setup modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4LdQl9Z_kFH",
        "outputId": "418d7f89-1130-4300-a5bb-c8e0ae29ad3a"
      },
      "source": [
        "! git clone https://github.com/OlgaSeleznova/ML_toolbox.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML_toolbox'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 90 (delta 40), reused 14 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (90/90), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsrSBxduwzrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd0dffc-5c8b-4963-d02f-27f5212a0dd7"
      },
      "source": [
        "#install modules\n",
        "import sys\n",
        "sys.path.insert(0,'/ML_toolbox/NLP_preprocessing')\n",
        "\n",
        "! pip install langid\n",
        "# ! pip install NLP_preprocessing"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langid in /usr/local/lib/python3.7/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from langid) (1.19.5)\n",
            "Requirement already satisfied: NLP_preprocessing in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from NLP_preprocessing) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from NLP_preprocessing) (4.41.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from NLP_preprocessing) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from NLP_preprocessing) (0.22.2.post1)\n",
            "Requirement already satisfied: deepdish in /usr/local/lib/python3.7/dist-packages (from NLP_preprocessing) (0.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from NLP_preprocessing) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->NLP_preprocessing) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->NLP_preprocessing) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->NLP_preprocessing) (5.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (56.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->NLP_preprocessing) (0.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->NLP_preprocessing) (1.0.1)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deepdish->NLP_preprocessing) (3.4.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->NLP_preprocessing) (3.10.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->NLP_preprocessing) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->NLP_preprocessing) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->NLP_preprocessing) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->NLP_preprocessing) (1.24.3)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->deepdish->NLP_preprocessing) (2.7.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->NLP_preprocessing) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->NLP_preprocessing) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R3lYMKss2Sh"
      },
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import re\n",
        "from ML_toolbox.NLP_preprocessing import English_preprocess   #custom module for English preprocessing\n",
        "import langid   # package to identify language of lyrics\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acpStEDV9E9k",
        "outputId": "64f4eed5-d88a-4431-d3b1-45361eeabd4f"
      },
      "source": [
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "lyrics = pd.read_parquet('/content/drive/My Drive/ML_toolbox/data/metrolyrics.parquet')\n",
        "lyrics.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49976, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf2I94ZlSPFD",
        "outputId": "5a66dd87-9f3d-43fe-bd61-c6159ee776e4"
      },
      "source": [
        "# identify language of the lyric\n",
        "lyrics['language'] = lyrics['lyrics'].apply(lambda x: langid.classify(x)[0])\n",
        "lyrics['language'].value_counts()[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "en    46332\n",
              "es     1381\n",
              "de      668\n",
              "fr      277\n",
              "it      269\n",
              "pt      123\n",
              "sw      118\n",
              "fi       84\n",
              "no       84\n",
              "sv       53\n",
              "Name: language, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGuWrlDnTS-i"
      },
      "source": [
        "Since English posts are 46k out of 49k posts total, and text classification for multiple languages should be prepared separately, we will strat with only English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGPlut8AS8P4",
        "outputId": "97a2b4d9-fc02-4521-c46b-f31aa8b8eaa6"
      },
      "source": [
        "lyric_en = lyrics[lyrics['language'] == 'en'].copy()\n",
        "lyric_en.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46332, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pog7zfm7k_iO"
      },
      "source": [
        "Use English preprocessing class from this repository to clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWR3ESVrTH2C"
      },
      "source": [
        "# initialize an object from custom class for English preprocessing\n",
        "cleaner = English_preprocess.English_preprocessing()\n",
        "# create a list of tuples to add custom cleaning with regex\n",
        "to_replace = [(r'[^a-zA-Z]',' '), (r'(\\s+)',' ')]\n",
        "# clean posts with regex\n",
        "lyric_en['lyrics_cleaned'] = lyric_en['lyrics'].apply(lambda x: cleaner.regex_cleaner(x,to_replace))\n",
        "# tokenize lyrics, remove stopwords and join to string.  \n",
        "lyric_en['lyrics_cleaned'] = lyric_en['lyrics_cleaned'].apply(lambda x: ' '.join(cleaner.tokenize(x)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXtR4DHh7w93"
      },
      "source": [
        "#encode genre names\n",
        "genre_srt_to_int = {'Rock':0, 'Pop':1, 'Hip-Hop':2, 'Metal':3, 'Country':4}\n",
        "lyric_en['genre'] = lyric_en['genre'].replace(genre_srt_to_int)\n",
        "\n",
        "# rename columns to more suitable\n",
        "lyric_en = lyric_en.rename(columns={'lyrics_cleaned':'text','genre':'label'})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W_pwq1akjLx"
      },
      "source": [
        "Now posts are cleaned and we can divide data into train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDDzhnR0Zl4l",
        "outputId": "fffb7257-1cc6-44d6-ffba-4caf09742a67"
      },
      "source": [
        "train_ratio = 0.75\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.10\n",
        "\n",
        "# train is now 75% of the entire data set\n",
        "x_train, x_test, y_train, y_test = train_test_split(lyric_en['text'], lyric_en['label'], test_size=1 - train_ratio, random_state=42)\n",
        "\n",
        "# test is now 10% of the initial data set\n",
        "# validation is now 15% of the initial data set\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42) \n",
        "\n",
        "print(x_train.shape, x_val.shape, x_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34749,) (6949,) (4634,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcpHrZK2kxnj"
      },
      "source": [
        "Save datasets to the folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FYJqEudZnOv"
      },
      "source": [
        "def create_datasets(x, y, file_name):\n",
        "    df = pd.concat([x, y], axis=1, ignore_index=False, sort=False)\n",
        "    print('Dataset shape: ',df.shape)\n",
        "    df.to_csv('/content/drive/My Drive/ML_toolbox/data/eng_lyrics/' + file_name, index=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmcJgdMiiGeu"
      },
      "source": [
        "! mkdir data\n",
        "! mkdir data/eng_lyrics/"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-zcmfrjaKZN",
        "outputId": "7dfc3867-94b2-4ffc-90c4-1547e744fab6"
      },
      "source": [
        "create_datasets(x_train, y_train, 'train_t.csv')\n",
        "create_datasets(x_val, y_val, 'valid_t.csv')\n",
        "create_datasets(x_test, y_test, 'test_t.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset shape:  (34749, 2)\n",
            "Dataset shape:  (6949, 2)\n",
            "Dataset shape:  (4634, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYO7LpR_WJJ7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}